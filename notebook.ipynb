{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling training and serving thousands of models with Amazon SageMaker\n",
    "\n",
    "As machine learning becomes increasingly prevalent in a wide range of industries, organizations are finding the need to train and serve large numbers of machine learning models to meet the diverse needs of their customers. For SaaS providers in particular, the ability to train and serve thousands of models efficiently and cost-effectively is crucial for staying competitive in a rapidly evolving market. \n",
    "\n",
    "Training and serving thousands of models requires a robust and scalable infrastructure, and this is where Amazon SageMaker (http://aws.amazon.com/sagemaker) can help. Amazon SageMaker is a fully-managed platform that enables developers and data scientists to build, train, and deploy machine learning models quickly, while also offering the cost-saving benefits of using Amazon's cloud infrastructure. \n",
    "\n",
    "In this blog post, we will explore how SageMaker's features, including SageMaker Processing, SageMaker Training Jobs, and Amazon SageMaker Multi-Model Endpoint, can be used to train and serve thousands of models in a cost-effective way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qr dev-requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the dataset\n",
    "\n",
    "For this blog post, we will play the role of an ISV company that helps their customers become more sustainable by tracking their energy consumption and providing forecasts. Our company has 1000 customers who want to better understand their energy usage and make informed decisions about how to reduce their environmental impact. To do this, we will use a synthetic dataset and train a machine learning model based on [Prophet](https://facebook.github.io/prophet/) for each customer to make energy consumption forecasts. Using Amazon SageMaker, we will be able to efficiently train and serve these 1000 models, providing our customers with accurate and actionable insights into their energy usage.\n",
    "\n",
    "There are three features in the generated dataset:\n",
    "\n",
    "* `customer_id`: This is an integer identifier for each customer, ranging from 0 to 999.\n",
    "* `timestamp`: This is a date-time value that indicates the time at which the energy consumption was measured. The timestamps are randomly generated between the start and end dates specified in the code.\n",
    "* `consumption`: This is a float value that indicates the energy consumption, measured in some arbitrary unit. The consumption values are randomly generated between 0 and 1000 with sinusoidal seasonality.\n",
    "\n",
    "If you already have a dataset, you can also leverage your own. Make sure the schema is the same as the one below: `(customer_id, timestamp, consumption)`, or change the code in the preprocessing script [data_splitter.py](./source/data_splitter.py).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source.data_generator as datagen\n",
    "\n",
    "!mkdir -p data\n",
    "output_path = datagen.main(\"2022-01-01 00:00:00\", \"2022-12-31 23:59:59\", 1000, \"energy_consumption.csv\", \"./data\")\n",
    "output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(output_path, parse_dates=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['customer_id']==0)][:100].plot(x='timestamp', y='consumption', style='o-', legend=False, colormap='viridis', figsize=(50,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to Amazon S3\n",
    "import sagemaker\n",
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session()\n",
    "execution_role = get_execution_role()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "prefix = 'sagemaker/' + name_from_base('scaling-thousand-models')\n",
    "raw_data = session.upload_data(output_path, bucket, f\"{prefix}/data\")\n",
    "print(f'Data uploaded to {raw_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing\n",
    "\n",
    "In order to leverage the same training job to create a thousand models, we will leverage the data distribution capability of the SageMaker SDK. When we launch a training job with 10 instances, and we configure the training data to be split across the training instances (thanks to the `distribution` parameter of the [`TrainingInput`](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.inputs.TrainingInput) class in the SageMaker Python SDK), SageMaker will round-robin data frmo the folder where the data is stored. Each instance will contain a subset of the full dataset. To make sure that each instance receives all the data for a single customer, we pre-process the data to create a CSV per customer, then store it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn import SKLearnProcessor\n",
    "\n",
    "processing_output = f\"s3://{bucket}/{prefix}/data/processed\"\n",
    "\n",
    "skp = SKLearnProcessor(\n",
    "    framework_version=\"1.0-1\",\n",
    "    role = execution_role, \n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    base_job_name=\"scaling-thousand-dataprep\",\n",
    "    sagemaker_session=session,\n",
    ")\n",
    "\n",
    "skp.run(\n",
    "    code=\"source/data_splitter.py\",\n",
    "    inputs=[ProcessingInput(source=raw_data, input_name=\"raw_data\", destination=\"/opt/ml/processing/input/\")],\n",
    "    outputs=[ProcessingOutput(output_name=\"customer_data\", source=\"/opt/ml/processing/output/customer_data/\", destination=processing_output)],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $processing_output/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indeed see what is the folder structure for the processed data. Customer data is stored in a specific file per each customer, and this will be round-robined to the different training instances.\n",
    "\n",
    "## Models Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We distribute the training over multiple instances, to do so we use the [`TrainingInput`](https://sagemaker.readthedocs.io/en/stable/api/utility/inputs.html#sagemaker.inputs.TrainingInput) class in SageMaker Python SDK. The `distribution` specifies how the training is distributed, `ShardedByS3Key` option means that the training data is sharded by S3 object key, with each training instance receiving a unique subset of the data, avoiding duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "\n",
    "processed_data_input = TrainingInput(\n",
    "    s3_data=processing_output,\n",
    "    distribution=\"ShardedByS3Key\",\n",
    "    content_type=\"text/csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    input_mode=\"File\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.mxnet import MXNet\n",
    "\n",
    "n_instances = 10\n",
    "\n",
    "checkpoint_local_path=\"/opt/ml/checkpoints\"\n",
    "training_output = f's3://{bucket}/{prefix}/models/'\n",
    "\n",
    "pt = MXNet(\n",
    "    entry_point='training.py',\n",
    "    source_dir='source',\n",
    "    role = execution_role,\n",
    "    framework_version='1.9.0',\n",
    "    py_version='py38',\n",
    "    instance_count=n_instances,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    base_job_name='scaling-thousand-training',\n",
    "    checkpoint_s3_uri=training_output,\n",
    "    checkpoint_local_path=checkpoint_local_path,\n",
    ")\n",
    "pt.fit(processed_data_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we can view the trained models in the `training_output` S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls $training_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model serving with MultiModel Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we define the [model object](https://sagemaker.readthedocs.io/en/stable/api/inference/model.html), to deploy on an endpoint later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn import SKLearnModel\n",
    "import os\n",
    "\n",
    "model = SKLearnModel(\n",
    "    model_data=os.path.join(training_output, \"1.tar.gz\"),\n",
    "    role=execution_role,\n",
    "    entry_point=\"inference.py\",\n",
    "    source_dir=\"source\",\n",
    "    framework_version=\"1.0-1\",\n",
    "    py_version=\"py3\",\n",
    "    name=name_from_base(\"scaling-thousand-sklearn-model\"),\n",
    "    sagemaker_session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we create the Multi-Model endpoint, this endpoint allows you to serve multiple models at the same time by creating an endpoint configuration that includes a list of all the models to serve, and then creating an endpoint using that endpoint configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.multidatamodel import MultiDataModel\n",
    "\n",
    "multimodel = MultiDataModel(\n",
    "    name=name_from_base('customer-models'),\n",
    "    model_data_prefix=training_output,\n",
    "    model=model,\n",
    "    sagemaker_session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can deploy the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = multimodel.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.c5.xlarge',\n",
    "    endpoint_name=name_from_base('scaling-models-sklearn'),\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the models are deployed we can use them for inference. Here we need to specify which model we want to use with the `target_model` parameter (i.e., model `8.tar.gz`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(data='{\"period\": 7}', target_model='8.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(response)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or model `735.tar.gz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = predictor.predict(data='{\"period\": 7}', target_model='735.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_json(response)[['ds', 'yhat', 'yhat_lower', 'yhat_upper']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "Remove the resources created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()\n",
    "multimodel.delete_model()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5df04d55f747e6ab181c22a19007192d5354ef00abedc523855ee058de521da4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
